# -*- coding: utf-8 -*-
"""1.1 간단하게 구현해보기

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-NMdfUrhOTfKk474RFG-V8jT6RlJt0-j
"""

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import pandas as pd
from tqdm import tqdm
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer

# device = "cuda" if torch.cuda.is_available() else "cpu"
# model.to(device)

from google.colab import drive
drive.mount('/content/drive')

"""# 긍*부정 분류"""

import pandas as pd
import numpy as np
import torch
from tqdm.auto import tqdm
import random
import os

def reset_seeds(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True

DATA_PATH = "/content/drive/MyDrive/멋쟁이사자차럼/data/"
SEED = 42

device = 'cuda' if torch.cuda.is_available() else 'cpu'
device

train = pd.read_csv(f"{DATA_PATH}review_train.csv")
test = pd.read_csv(f"{DATA_PATH}review_test.csv")

train.shape, test.shape
train
train.info()

# Commented out IPython magic to ensure Python compatibility.
# %pip install kiwipiepy
from kiwipiepy import Kiwi
text = train["review"][0]
text

kiwi = Kiwi()
result = kiwi.tokenize(train["review"])
train_list = [ [ t.form for t in tokens ] for tokens in result ]
result = kiwi.tokenize(test["review"])
test_list = [ [ t.form for t in tokens ] for tokens in result ]
from sklearn.feature_extraction.text import TfidfVectorizer
vec = TfidfVectorizer(max_features=500)
train_data = vec.fit_transform(
    [ " ".join(t) for t in train_list ]
).A
test_data = vec.transform(
    [ " ".join(t) for t in test_list ]
).A
train_data.shape, test_data.shape
(train_data.sum(axis=1) == 0).sum()
target = train["target"].to_numpy().reshape(-1, 1)
target.shape

"""## 데이터셋"""

class ReviewDataset(torch.utils.data.Dataset):
    def __init__(self, x, y=None):
        self.x , self.y = x, y

    def __len__(self):
        return self.x.shape[0]

    def __getitem__(self, i):
        item = {}
        item["x"] = torch.Tensor(self.x[i])
        if self.y is not None:
            item["y"] = torch.Tensor(self.y[i])
        return item
dt = ReviewDataset(train_data,target)
dl= torch.utils.data.DataLoader(dt, batch_size=2)
batch = next(iter(dl))
batch

"""## 모델 클래스"""

class ResidualBlock(torch.nn.Module):
    def __init__(self, in_features):
        super().__init__()
        self.fx = torch.nn.Sequential(
            torch.nn.Linear(in_features, in_features),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.5),
            torch.nn.Linear(in_features, in_features)
        )
        self.relu = torch.nn.ReLU()

    def forward(self, x):
        fx = self.fx(x)
        hx = fx + x
        return self.relu(hx)
class Net(torch.nn.Module):
    def __init__(self, in_features, n_layers=8):
        super().__init__()

        self.init_layer = torch.nn.Sequential(
            torch.nn.Linear(in_features, in_features // 2),
            torch.nn.BatchNorm1d(in_features // 2),
            torch.nn.LeakyReLU()
        )
        res_list = [ ResidualBlock(in_features//2) for _ in range(n_layers) ]
        self.seq = torch.nn.Sequential(*res_list)
        self.output_layer = torch.nn.Linear(in_features//2, 1)

    def forward(self, x):
        x = self.init_layer(x)
        x = self.seq(x)
        return self.output_layer(x)
Net(train_data.shape[1])(batch["x"])

"""## 학습 루프"""

def train_loop(dl, model, loss_fn, optimizer, device):
    epoch_loss = 0
    model.train()
    for batch in dl:
        pred = model(batch["x"].to(device))
        loss = loss_fn(pred, batch["y"].to(device))

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()

    epoch_loss /= len(dl)
    return epoch_loss

"""## 테스트 루프"""

@torch.no_grad()
def test_loop(dl, model, loss_fn, device):
    epoch_loss = 0
    model.eval()

    act = torch.nn.Sigmoid()
    pred_list = []
    for batch in dl:
        pred = model( batch["x"].to(device) )
        if batch.get("y") is not None:
            loss = loss_fn(pred, batch["y"].to(device) )
            epoch_loss += loss.item()

        pred = act(pred)
        pred = pred.to("cpu").numpy()
        pred_list.append(pred)

    pred = np.concatenate(pred_list)
    epoch_loss /= len(dl)
    return epoch_loss, pred

"""## 학습하기"""

n_splits = 5
batch_size = 32
epochs = 100
loss_fn = torch.nn.BCEWithLogitsLoss()

from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold
cv = KFold(n_splits, shuffle=True, random_state=SEED)
is_holdout = False
reset_seeds(SEED)
score_list = []
for i, (tri, vai) in enumerate(cv.split(train_data)):
    # 학습 데이터
    train_dt = ReviewDataset(train_data[tri], target[tri])
    train_dl = torch.utils.data.DataLoader(train_dt, batch_size=batch_size, shuffle=True)
    # 검증 데이터
    valid_dt = ReviewDataset(train_data[vai], target[vai])
    valid_dl = torch.utils.data.DataLoader(valid_dt, batch_size=batch_size, shuffle=False)

    # 모델 객체 및 옵티마이저 생성
    model = Net(train_data.shape[1]).to(device)
    optimizer = torch.optim.Adam( model.parameters() )

    patience = 0 # 조기 종료 조건을 주기 위한 변수
    best_score = 0 # 현재 최고점수
    for _ in tqdm(range(epochs)):
        train_loss = train_loop(train_dl, model, loss_fn, optimizer, device)
        valid_loss, pred = test_loop(valid_dl, model, loss_fn, device)
        pred = np.where(pred > 0.5, 1, 0)
        score = accuracy_score(target[vai], pred)
        print(train_loss, valid_loss, score)

        patience += 1
        if score > best_score:
            best_score = score
            patience = 0
            torch.save( model.state_dict(), f"model{i}.pt" )

        if patience == 10:
            break

    score_list.append(best_score)
    print(f"ACC 최고점수: {best_score}")

    if is_holdout:
        break
print(score_list)
print(np.mean(score_list))

"""## 예측하기"""

test_dt = ReviewDataset(test_data)
test_dl = torch.utils.data.DataLoader(test_dt, shuffle=False, batch_size=batch_size)
pred_list = []
for i in range(n_splits):
    model = Net(train_data.shape[1]).to(device)
    state_dict = torch.load(f"model{i}.pt", weights_only=True)
    model.load_state_dict(state_dict)

    _, pred = test_loop(test_dl, model, None, device)
    pred_list.append(pred)
pred = np.mean(pred_list, axis=0)
pred = np.where(pred>0.5, 1, 0)
pred.shape

"""# 맞춤법 교정한 파일 가져오기"""

# 3. 엑셀 파일에서 데이터 읽기
file_path = '/content/drive/MyDrive/멋쟁이사자차럼/data/신흥시장_맞춤법교정완료파일.xlsx'
df = pd.read_excel(file_path)

len(df)

df

df['clean_review']= df['review'].str.replace("[^\w ]+", "", regex=True).str.lower()

filtered_data =df[df['clean_review'].str.len() >= 3]  # 길이 3미만 리뷰 제거

"""- 한글 리뷰만 남기고 다 제거해보기"""

import re  # re 모듈 임포트

def extract_word(text):
    hangul = re.compile('[^가-힣]')
    result = hangul.sub(' ', text)
    return result

df['clean_review'] = df['clean_review'].apply(lambda x:extract_word(x))

"""## 키워드 추출"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install konlpy
from konlpy.tag import Okt, Komoran, Hannanum, Kkma

okt = Okt()
words = " ".join(df['clean_review'].tolist())
words = okt.morphs(words,stem=True)

len(words)

# 한글자 제거
remove_one_word = [x for x in words if len(x)>1 ]
len(remove_one_word)

# 단어별 빈도를 확인
from collections import Counter
frequent = Counter(remove_one_word).most_common()

frequent

"""- 불용어 좀 더 상의해서 제거해야할듯.
- 일단은 러프하게 그냥 하자.
"""

with open('/content/drive/MyDrive/멋쟁이사자차럼/data/stopwords.txt', 'r', encoding = 'CP949') as f:
    list_file = f.read()

# 불용어 리스트 생성
stopwords = list_file.split(",")  # 쉼표로 나누기

# 불용어 제거
remove_stopwords = [x for x in remove_one_word if x not in stopwords]
len(remove_stopwords)

Counter(remove_stopwords).most_common()

"""- 빈도수 적은 단어들 보기"""

for item, count in Counter(remove_stopwords).most_common():
    if count==1:
        print(item)

"""- 3회 이하로 나온 단어들 일단 삭제!! .. 이것도 상의해봐야 할듯하다."""

minimum_count = 3
more_than_one_time= []
for i in tqdm(range(len(remove_stopwords))):
    tmp = remove_stopwords[i]
    if remove_stopwords.count(tmp) >= minimum_count:
        more_than_one_time.append(tmp)

len(more_than_one_time)

text = set(more_than_one_time)

# CountVectorizer 객체 생성 (필요에 따라 추가 파라미터 조정 가능)
vectorizer = CountVectorizer()

# 리뷰 데이터를 CountVectorizer로 변환
data_features = vectorizer.fit_transform(df['clean_review'].tolist())

# TF-IDF 변환기 생성 및 데이터 변환
tfidf_vectorizer = TfidfTransformer()
tf_idf_vect = tfidf_vectorizer.fit_transform(data_features)

# 단어 인덱스와 단어를 매핑하는 사전 생성
vocab = {v: k for k, v in vectorizer.vocabulary_.items()}

# 출력 확인
print("단어와 인덱스 매핑:", vocab)
print("TF-IDF Matrix Shape:", tf_idf_vect.shape)

"""- 본격 키워드 추출
- 이 사이에 긍부정으로 나누는 과정 있으면 좋을 듯 하다.

- 그 다음에 긍정, 부정 별로 키워드... 왜 이 걸 선택했는지... 카페별로! 있으면 좋을듯.
- 나는 지금 전체에 대해서 했지만, 카페별로 해야될듯 하다 ^.^
- 즉 하... 아냐 걍 여기서 멈춰..................
"""